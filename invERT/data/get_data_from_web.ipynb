{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nollyverse download and cleaning\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from tarfile import open as taropen\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions\n",
    "This function try to reach the THREDDS (THematic Real-time Environmental Distributed Data Services) Data Server (TDS). The THREDDS project provide access to datasets. Here, HTTP download is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_url_from_thredds(\n",
    "        catalog_url: str,\n",
    "        model_tarname: str\n",
    "    )-> str:\n",
    "    \"\"\"Fetch the correct tar file URL from the THREDDS catalog.\"\"\"\n",
    "    response: requests.Response = requests.get(catalog_url)\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Failed to access catalog: {catalog_url}\")\n",
    "\n",
    "    root: ET.Element = ET.fromstring(response.content)\n",
    "\n",
    "    # Define the namespace\n",
    "    namespace: dict[str, str] = {\"ns\": \"http://www.unidata.ucar.edu/namespaces/thredds/InvCatalog/v1.0\"}\n",
    "\n",
    "    # Find all dataset elements\n",
    "    datasets: list[ET.Element] = root.findall(\".//ns:dataset\", namespaces=namespace)\n",
    "\n",
    "    # Extract the URL path for each dataset\n",
    "    available_files: list[str] = [dataset.attrib.get(\"urlPath\", \"\") for dataset in datasets]\n",
    "\n",
    "    for url_path in available_files:\n",
    "        if model_tarname in url_path:  # Match the expected tar filename\n",
    "            return f\"https://thredds.nci.org.au/thredds/fileServer/{url_path}\"\n",
    "\n",
    "    raise ValueError(f\"File {model_tarname} not found in THREDDS catalog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the URL, we can download the tarfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_tar(\n",
    "        tar_url: str,\n",
    "        download_path: Path,\n",
    "        overwrite: bool = False,\n",
    "    ) -> Path | None:  \n",
    "    \"\"\"Download a tar file.\"\"\"\n",
    "    print(f\"Downloading tar file from {tar_url}.\")\n",
    "\n",
    "    # Create the download path if it does not exist\n",
    "    try:\n",
    "        download_path.mkdir(parents=True, exist_ok=False)\n",
    "        print(f\"Successfully created directory {download_path}.\")\n",
    "    except FileExistsError:\n",
    "        if not overwrite:\n",
    "            print(f\"Directory {download_path} already exists. To overwrite it, make sure to set the overwrite flag to True. If you don't want to overwrite, please provide a different path.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Potentially overwriting directory {download_path} ...\")\n",
    "\n",
    "    # Download the tar file by chunks (large file)\n",
    "    response: requests.Response = requests.get(tar_url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Downloaded file name\n",
    "    tar_filename: Path = download_path / Path(tar_url).name\n",
    "\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    print(f\"Downloading to {tar_filename} ...\")\n",
    "\n",
    "    # Download the tar file by chunks of chunk_size bytes\n",
    "    with open(tar_filename, \"wb\") as file, tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\") as progress_bar:\n",
    "        for chunk in response.iter_content(chunk_size=65536):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                progress_bar.update(len(chunk))\n",
    "                \n",
    "\n",
    "    return tar_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, extract it (~1 min per 500 MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar(\n",
    "        tar_filename: Path,\n",
    ") -> None:\n",
    "    # Extract contents\n",
    "    print(f\"Extracting tar file {tar_filename} ...\")\n",
    "    with taropen(tar_filename, \"r\") as tar:\n",
    "        members = tar.getmembers()\n",
    "        for member in tqdm(tar, desc=\"Extraction\", unit=\"file\", total=len(members)):\n",
    "            tar.extract(member, path=tar_filename.parent)\n",
    "    print(f\"Successfully extracted tar file {tar_filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "Here, determine the succession of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENTS: dict[int, str] = {\n",
    "    1: \"FOLD\",\n",
    "    2: \"FAULT\",\n",
    "    3: \"UNCONFORMITY\",\n",
    "    4: \"SHEAR-ZONE\",\n",
    "    5: \"DYKE\",\n",
    "    6: \"PLUG\",\n",
    "    7: \"TILT\",\n",
    "}\n",
    "events_list: list[int] = [3, 6, 7]\n",
    "assert(len(events_list) == 3, \"len(events_list) must be 3.\")\n",
    "\n",
    "events: str = f\"{EVENTS[events_list[0]]}_{EVENTS[events_list[1]]}_{EVENTS[events_list[2]]}\"\n",
    "tar_name: str = f\"{events}.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the dataset local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder: Path = Path(\"../../../dataset/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get the URL of the tarfile for this succession of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_url: str=\"https://thredds.nci.org.au/thredds/catalog/tm64/noddyverse/bulk_models/catalog.xml\"\n",
    "tar_url: str = get_file_url_from_thredds(catalog_url, tar_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download the tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tar file from https://thredds.nci.org.au/thredds/fileServer/tm64/noddyverse/bulk_models/UNCONFORMITY_PLUG_TILT.tar.\n",
      "Potentially overwriting directory ..\\..\\..\\dataset\\test ...\n",
      "Downloading to ..\\..\\..\\dataset\\test\\UNCONFORMITY_PLUG_TILT.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 550M/550M [12:44<00:00, 720kB/s]   \n"
     ]
    }
   ],
   "source": [
    "tar_filename: Path | None = download_tar(tar_url, dataset_folder, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Extract the tarfile content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tar file ..\\..\\..\\dataset\\test\\UNCONFORMITY_PLUG_TILT.tar ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|██████████| 5066/5066 [01:00<00:00, 83.80file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted tar file ..\\..\\..\\dataset\\test\\UNCONFORMITY_PLUG_TILT.tar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_tar(tar_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Delete every file that is not '.g12.gz' (geological model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path(dataset_folder) / \"models_by_code/models\" / events\n",
    "files = filepath.glob(\"*\")\n",
    "\n",
    "for file in files:\n",
    "    if not file.suffixes == [\".g12\", \".gz\"]:  # Correctly checking both suffixes\n",
    "        file.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
